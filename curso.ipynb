{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "curso.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gamallo/jupiter/blob/main/curso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e10f0ab5"
      },
      "source": [
        "# NLP Scripts to Build a Language Model and Compute Word Similarity \n",
        "In this demo, all scripts used to generate a language model and to compare the semantic similarity between words are introduced. This is the pipeline:\n",
        "\n",
        "* **Building the language model**:\n",
        "\n",
        "*Input*: text corpus \n",
        "\n",
        "*Process*: `Tokenization -> N-grams -> Stopwords removal -> Vectorization` \n",
        "\n",
        "*Output*: language model\n",
        "\n",
        "* **Word Similarity**:\n",
        "\n",
        "*Input*: Word pairs + Language Model \n",
        "\n",
        "*Process*: `Cosine Similarity` \n",
        "\n",
        "*Output*: list of similar words"
      ],
      "id": "e10f0ab5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nV852ssP9wA",
        "outputId": "b5004031-30b7-4dd8-a566-d422ff94a251"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "3nV852ssP9wA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bf32dd1"
      },
      "source": [
        "# Building the Language Model\n",
        "\n",
        "## Tokenizer\n",
        "A tokenizer breaks a stream of text into tokens, usually by looking for whitespace (tabs, spaces, new lines).\n",
        "\n",
        "Substitution function (re.sub) takes three arguments:\n",
        "* substring to be replaced : `(\\w)([\\,\\.])`\n",
        "* replaced substring : `\\1 \\2`\n",
        "* string containing the substring to be replaced: `line`\n",
        "\n",
        "Let us start by a simple example:"
      ],
      "id": "7bf32dd1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ba0db20",
        "outputId": "4084a6ec-a31a-4aa3-a335-3cb1b61c2d6d"
      },
      "source": [
        "import re\n",
        "\n",
        "line = input()\n",
        "print(re.sub(r\"(\\w)([\\,\\.])\",r\"\\1 \\2\",line))"
      ],
      "id": "6ba0db20",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llç\n",
            "llç\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiCiRhvAlh4y"
      },
      "source": [
        "## Input / Output\n",
        "Now, we introduce the input and output files. The tokenizer script opens and reads an input file (flag: `r`), and the tokenized text is writen in an output file (flag: `w`)\n"
      ],
      "id": "DiCiRhvAlh4y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f88fbe5d",
        "outputId": "78179b68-a17d-40d5-db6c-ef0b76afedd2"
      },
      "source": [
        "import re\n",
        "#file1 = open('/content/drive/MyDrive/emlex/data/corpus.txt', 'r')\n",
        "file1 = open('corpus.txt', 'r')\n",
        "output=\"\"\n",
        "for line in file1:\n",
        "    #line = line.strip()\n",
        "    line = re.sub(r\"(\\w)([\\,\\.])\",r\"\\1 \\2\",line)\n",
        "    output += line\n",
        "\n",
        "file2 = open('tokens.txt', 'w')\n",
        "file2.write(output)"
      ],
      "id": "f88fbe5d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "397"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fff60e20"
      },
      "source": [
        "## N-grams\n",
        "The script takes the file with the tokens and generates a new file with n-grams (the default parameter is 3, thus, it generates trigrams). It contain the function `ngrams` defined using other functions on lists such as `len` and `range`.\n",
        "\n",
        "`ngrams.py` is a python file stored in our google drive folder. It generates n-grams (`trigrams.txt`) from the `tokens.txt` file"
      ],
      "id": "fff60e20"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNoPsJR5TWaM",
        "outputId": "e8d0ed98-abe5-4918-9bcc-da5bf87b3804"
      },
      "source": [
        "size = 3 ##trigrams\n",
        "\n",
        "##open file tokens:\n",
        "file_tokens = open('tokens.txt', 'r')\n",
        "\n",
        "def ngrams(input, n): ##main function\n",
        "  input = input.split(' ')\n",
        "  result = []\n",
        "  for i in range(len(input)-n+1):  ##range from 0 to position of the first element of the last ngram\n",
        "    result.append(input[i:i+n])\n",
        "  return result\n",
        "\n",
        "output=\"\"\n",
        "for line in file_tokens:  \n",
        "  line = line.strip()\n",
        "\n",
        "  if len(line)>1:\n",
        "    result = ngrams(line,size) \n",
        "    for ngram in result:\n",
        "      juntar = \"\"\n",
        "      for token in ngram:\n",
        "        juntar += token + \" \"\n",
        "      output += juntar + \"\\n\"\n",
        "      print (juntar)\n",
        "\n",
        "file2 = open('trigrams.txt', 'w')\n",
        "file2.write(output)"
      ],
      "id": "uNoPsJR5TWaM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pedro read books \n",
            "read books and \n",
            "books and Maria \n",
            "and Maria read \n",
            "Maria read books \n",
            "read books too \n",
            "books too , \n",
            "Pedro read novels \n",
            "read novels and \n",
            "novels and Maria \n",
            "and Maria read \n",
            "Maria read novels \n",
            "read novels and \n",
            "novels and books \n",
            "and books , \n",
            "Pedro and Maria \n",
            "and Maria read \n",
            "Maria read many \n",
            "read many things \n",
            "many things , \n",
            "things , but \n",
            ", but Pedro \n",
            "but Pedro loves \n",
            "Pedro loves Maria \n",
            "loves Maria , \n",
            "Maria loves books \n",
            "loves books , \n",
            "books , in \n",
            ", in fact \n",
            "in fact Maria \n",
            "fact Maria loves \n",
            "Maria loves many \n",
            "loves many things \n",
            "many things . \n",
            "Maria is eating \n",
            "is eating an \n",
            "eating an apple \n",
            "an apple and \n",
            "apple and Pedro \n",
            "and Pedro is \n",
            "Pedro is eating \n",
            "is eating an \n",
            "eating an apple \n",
            "an apple too \n",
            "apple too , \n",
            "Pedro is eating \n",
            "is eating eggs \n",
            "eating eggs now \n",
            "eggs now , \n",
            "now , Pedro \n",
            ", Pedro and \n",
            "Pedro and Maria \n",
            "and Maria are \n",
            "Maria are eating \n",
            "are eating many \n",
            "eating many things \n",
            "many things , \n",
            "Maria is eating \n",
            "is eating eggs \n",
            "eating eggs , \n",
            "eggs , Maria \n",
            ", Maria and \n",
            "Maria and Pedro \n",
            "and Pedro loves \n",
            "Pedro loves eggs \n",
            "loves eggs a \n",
            "eggs a lot \n",
            "a lot . \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1074"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9ff3dea"
      },
      "source": [
        "!python3 /content/drive/MyDrive/emlex/ngrams.py 3"
      ],
      "id": "a9ff3dea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLmI2YKOeH4N"
      },
      "source": [
        "## StopWords\n",
        "\n",
        "The following task is to identify grammar words (stopwords) from the textual n-grams. For this purpose, we need a file with a list of stopwords: `stopwords-en.txt`\n"
      ],
      "id": "eLmI2YKOeH4N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEGDfLFcgixr",
        "outputId": "40e4343c-8381-4d72-c288-1c26936cfd31"
      },
      "source": [
        "##open file trigrams:\n",
        "file_trigrams = open('trigrams.txt', 'r')\n",
        "##open file stopwords:\n",
        "#file_stopwords = open('/content/drive/MyDrive/emlex/data/stopwords-en.txt', 'r')\n",
        "file_stopwords = open('stopwords-en.txt', 'r')\n",
        "\n",
        "stop=[] ##this is the list where stopwords will be stored\n",
        "for token in file_stopwords:\n",
        "  token = token.strip()\n",
        "  stop.append(token)\n",
        "\n",
        "##read the trigrams and store each position token in a variable:\n",
        "output=\"\"\n",
        "for line in file_trigrams:\n",
        "  line = line.strip() \n",
        "  line = line.split() ##from string to list\n",
        "  if len(line) >= 3:\n",
        "    w1 = line[0]\n",
        "    w2 = line[1]\n",
        "    w3 = line[2]\n",
        "\n",
        "##check if the tokens are in the list of stopwords, called stop:\n",
        "  if w1 in stop:\n",
        "    w1 = \"STOP\"\n",
        "  if w2 in stop:\n",
        "    w2 = \"STOP\"\n",
        "  if w3 in stop:\n",
        "    w3 = \"STOP\"\n",
        "\n",
        "  print(w1, w2, w3)\n",
        "  output += w1 + \" \" + w2 + \" \" + w3 + \"\\n\"\n",
        "\n",
        "file2 = open('trigrams2.txt', 'w')\n",
        "file2.write(output)\n",
        "\n"
      ],
      "id": "eEGDfLFcgixr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pedro read books\n",
            "read books STOP\n",
            "books STOP Maria\n",
            "STOP Maria read\n",
            "Maria read books\n",
            "read books STOP\n",
            "books STOP STOP\n",
            "Pedro read novels\n",
            "read novels STOP\n",
            "novels STOP Maria\n",
            "STOP Maria read\n",
            "Maria read novels\n",
            "read novels STOP\n",
            "novels STOP books\n",
            "STOP books STOP\n",
            "Pedro STOP Maria\n",
            "STOP Maria read\n",
            "Maria read STOP\n",
            "read STOP things\n",
            "STOP things STOP\n",
            "things STOP STOP\n",
            "STOP STOP Pedro\n",
            "STOP Pedro loves\n",
            "Pedro loves Maria\n",
            "loves Maria STOP\n",
            "Maria loves books\n",
            "loves books STOP\n",
            "books STOP STOP\n",
            "STOP STOP fact\n",
            "STOP fact Maria\n",
            "fact Maria loves\n",
            "Maria loves STOP\n",
            "loves STOP things\n",
            "STOP things STOP\n",
            "Maria STOP eating\n",
            "STOP eating STOP\n",
            "eating STOP apple\n",
            "STOP apple STOP\n",
            "apple STOP Pedro\n",
            "STOP Pedro STOP\n",
            "Pedro STOP eating\n",
            "STOP eating STOP\n",
            "eating STOP apple\n",
            "STOP apple STOP\n",
            "apple STOP STOP\n",
            "Pedro STOP eating\n",
            "STOP eating eggs\n",
            "eating eggs STOP\n",
            "eggs STOP STOP\n",
            "STOP STOP Pedro\n",
            "STOP Pedro STOP\n",
            "Pedro STOP Maria\n",
            "STOP Maria STOP\n",
            "Maria STOP eating\n",
            "STOP eating STOP\n",
            "eating STOP things\n",
            "STOP things STOP\n",
            "Maria STOP eating\n",
            "STOP eating eggs\n",
            "eating eggs STOP\n",
            "eggs STOP Maria\n",
            "STOP Maria STOP\n",
            "Maria STOP Pedro\n",
            "STOP Pedro loves\n",
            "Pedro loves eggs\n",
            "loves eggs STOP\n",
            "eggs STOP lot\n",
            "STOP lot STOP\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1141"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brtjVAnmJBhH"
      },
      "source": [
        "# Language Model\n",
        "We will build a language model of ngrams (trigrams). This consists of representing words as vectors in a word-context matrix (vectorization). This is performed in several steps:\n",
        "\n",
        "## Matrix from trigrams\n",
        "\n",
        "By using the final file of trigrams (`trigrams2.txt`), a dictionary with the frequency of each word in a word context is created. Each line of the dictionary is a triple `(word,context,frequency)`\n"
      ],
      "id": "brtjVAnmJBhH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ywMnbmzKxFX",
        "outputId": "d27d9100-9597-4103-86b8-bc2b32cf3d59"
      },
      "source": [
        "import sys\n",
        "from collections import defaultdict\n",
        "matrix = defaultdict(int) ##initializing a dictionary\n",
        "\n",
        "##open file trigrams2:\n",
        "file_trigrams = open('trigrams2.txt', 'r')\n",
        "\n",
        "for line in file_trigrams:  \n",
        "  line = line.strip()\n",
        "  line = line.split()\n",
        "  if len(line) >= 3:\n",
        "    w1 = line[0]\n",
        "    w2 = line[1]\n",
        "    w3 = line[2]\n",
        "\n",
        "##create the dictionary 'matrix' with a double key (word, context) and \n",
        "##a frequency value:\n",
        "  if w1 != \"STOP\" and w2 != \"STOP\":\n",
        "    matrix[w1,w2] += 1\n",
        "    matrix[w2,w1] += 1\n",
        "\n",
        "  if w1 != \"STOP\" and w3 != \"STOP\":\n",
        "    matrix[w1,w3] += 1\n",
        "    matrix[w3,w1] += 1\n",
        "\n",
        "file2 = open('matrix.txt','w') ##output file\n",
        "sys.stdout = file2  ##print stdout is in the ouput file\n",
        "\n",
        "for w,c in matrix:\n",
        "  print ('%s\\t%s\\t%d' % (w, c, matrix[w,c]))\n",
        "  print (w, c, matrix[w,c], file=sys.stderr)"
      ],
      "id": "5ywMnbmzKxFX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pedro read 2\n",
            "read Pedro 2\n",
            "Pedro books 1\n",
            "books Pedro 1\n",
            "read books 2\n",
            "books read 2\n",
            "books Maria 3\n",
            "Maria books 3\n",
            "Maria read 3\n",
            "read Maria 3\n",
            "Pedro novels 1\n",
            "novels Pedro 1\n",
            "read novels 2\n",
            "novels read 2\n",
            "novels Maria 2\n",
            "Maria novels 2\n",
            "novels books 1\n",
            "books novels 1\n",
            "Pedro Maria 4\n",
            "Maria Pedro 4\n",
            "read things 1\n",
            "things read 1\n",
            "Pedro loves 2\n",
            "loves Pedro 2\n",
            "loves Maria 3\n",
            "Maria loves 3\n",
            "loves books 1\n",
            "books loves 1\n",
            "fact Maria 1\n",
            "Maria fact 1\n",
            "fact loves 1\n",
            "loves fact 1\n",
            "loves things 1\n",
            "things loves 1\n",
            "Maria eating 3\n",
            "eating Maria 3\n",
            "eating apple 2\n",
            "apple eating 2\n",
            "apple Pedro 1\n",
            "Pedro apple 1\n",
            "Pedro eating 2\n",
            "eating Pedro 2\n",
            "eating eggs 2\n",
            "eggs eating 2\n",
            "eating things 1\n",
            "things eating 1\n",
            "eggs Maria 1\n",
            "Maria eggs 1\n",
            "Pedro eggs 1\n",
            "eggs Pedro 1\n",
            "loves eggs 1\n",
            "eggs loves 1\n",
            "eggs lot 1\n",
            "lot eggs 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcFivVjnb8Hh"
      },
      "source": [
        "## Punctual Mutual Information (PMI)\n",
        "\n",
        "For each `(word, context, frequency)` triple, PMI is computed. It results in the same matrix with PMI values instead of frequencies. "
      ],
      "id": "gcFivVjnb8Hh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2RLHQUNca4m",
        "outputId": "ae81d6ea-e6f7-49b2-9c5a-ee6b804e635f"
      },
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "##initializing three dictionaries and two variables:\n",
        "pair = defaultdict(int)  \n",
        "w = defaultdict(int)\n",
        "c = defaultdict(int)\n",
        "n=0;\n",
        "fr=0;\n",
        "\n",
        "##computing punctual mutual information by defining function pmi\n",
        "def pmi (joint, wo, co):\n",
        "  if  n:\n",
        "      output = math.log  ( (joint / n)  /  ( (w[wo]/n)*(c[co]/n) )  ) \n",
        "  else:\n",
        "      output = 0\n",
        "  return output\n",
        "\n",
        "##open input file matrix:\n",
        "file_matrix = open('matrix.txt', 'r')\n",
        "\n",
        "for line in file_matrix:  \n",
        "  line = line.strip()\n",
        "  line = line.split()\n",
        "  if len(line) >= 3:\n",
        "    word = line[0]\n",
        "    context = line[1]\n",
        "    fr =  float(line[2]) ##as the number is read as a string, \n",
        "                        ## it is converted into float\n",
        "        \n",
        "    w[word] += fr\n",
        "    c[context] += fr\n",
        "    pair[word,context] = fr        \n",
        "    n += fr\n",
        "\n",
        "file2 = open('matrix_pmi.txt','w') ##output file\n",
        "sys.stdout = file2  ##print stdout is in the ouput file\n",
        "\n",
        "for word,context in sorted(pair):\n",
        "  assoc = pmi(pair[word,context],word,context)\n",
        "  if assoc > 0:\n",
        "    print ('%s\\t%s\\t%.6f' % (word, context, assoc))\n",
        "    print (word, context, assoc, file=sys.stderr)"
      ],
      "id": "f2RLHQUNca4m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Maria Pedro 0.27329333499968117\n",
            "Maria books 0.545227050483323\n",
            "Maria eating 0.3220834991691132\n",
            "Maria fact 0.8329091229351039\n",
            "Maria loves 0.42744401482693956\n",
            "Maria novels 0.42744401482693956\n",
            "Maria read 0.3220834991691132\n",
            "Pedro Maria 0.27329333499968117\n",
            "Pedro apple 0.784118958765672\n",
            "Pedro eating 0.27329333499968117\n",
            "Pedro eggs 0.09097177820572659\n",
            "Pedro loves 0.3786538506575074\n",
            "Pedro novels 0.09097177820572659\n",
            "Pedro read 0.27329333499968117\n",
            "apple Pedro 0.784118958765672\n",
            "apple eating 1.8137383759468302\n",
            "books Maria 0.545227050483323\n",
            "books loves 0.2451224580329849\n",
            "books novels 0.6505875661411494\n",
            "books read 0.8329091229351039\n",
            "eating Maria 0.3220834991691132\n",
            "eating Pedro 0.27329333499968117\n",
            "eating apple 1.8137383759468302\n",
            "eating eggs 1.1205911953868848\n",
            "eating things 1.1205911953868848\n",
            "eggs Pedro 0.09097177820572659\n",
            "eggs eating 1.1205911953868848\n",
            "eggs lot 2.7300291078209855\n",
            "eggs loves 0.5328045304847658\n",
            "fact Maria 0.8329091229351039\n",
            "fact loves 1.6314168191528755\n",
            "lot eggs 2.7300291078209855\n",
            "loves Maria 0.42744401482693956\n",
            "loves Pedro 0.3786538506575074\n",
            "loves books 0.2451224580329849\n",
            "loves eggs 0.5328045304847658\n",
            "loves fact 1.6314168191528755\n",
            "loves things 1.2259517110447111\n",
            "novels Maria 0.42744401482693956\n",
            "novels Pedro 0.09097177820572659\n",
            "novels books 0.6505875661411494\n",
            "novels read 1.1205911953868848\n",
            "read Maria 0.3220834991691132\n",
            "read Pedro 0.27329333499968117\n",
            "read books 0.8329091229351039\n",
            "read novels 1.1205911953868848\n",
            "read things 1.1205911953868848\n",
            "things eating 1.1205911953868848\n",
            "things loves 1.2259517110447111\n",
            "things read 1.1205911953868848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Op1qNVPeXBK"
      },
      "source": [
        "## Filtering\n",
        "\n",
        "We can select the most relevant contexts for each word. This is done by selecting only the `N` most relevant contexts. For this purpose, it is necessary to sort them by `pmi` value. At the end of this process, we obtain the final language model that will be used to compute word similarity."
      ],
      "id": "0Op1qNVPeXBK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWLkdMM4fKba",
        "outputId": "6f2008fc-dadf-4a24-ddce-0f7d31e32115"
      },
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "#th = int(sys.argv[1])\n",
        "th = 3\n",
        "w = defaultdict(dict)\n",
        "\n",
        "##open input file matrix_pmi:\n",
        "file_matrix_pmi = open('matrix_pmi.txt', 'r')\n",
        "\n",
        "for line in file_matrix_pmi:  \n",
        "  line = line.strip()  ## chomp $line\n",
        "  line = line.split()\n",
        "  if len(line) >= 3:\n",
        "    word = line[0]\n",
        "    context = line[1]\n",
        "    pmi =  float (line[2])\n",
        "\n",
        "    ##pmi value is stored in a dictionary of a dictionary\n",
        "    w[word][context] = pmi\n",
        "\n",
        "file2 = open('matrix_pmi_filtered.txt','w') ##output file\n",
        "sys.stdout = file2  ##print stdout is in the ouput file\n",
        "\n",
        "for word,contexts in sorted(w.items() ):\n",
        "  i=1\n",
        "  for context in sorted(contexts,key=contexts.get,reverse=True):\n",
        "    if i <= th:\n",
        "      print (word, context, w[word][context], file=sys.stderr)\n",
        "      print ('%s\\t%s\\t%.6f' % (word, context, w[word][context]))\n",
        "      i += 1\n"
      ],
      "id": "WWLkdMM4fKba",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Maria fact 0.832909\n",
            "Maria books 0.545227\n",
            "Maria loves 0.427444\n",
            "Pedro apple 0.784119\n",
            "Pedro loves 0.378654\n",
            "Pedro Maria 0.273293\n",
            "apple eating 1.813738\n",
            "apple Pedro 0.784119\n",
            "books read 0.832909\n",
            "books novels 0.650588\n",
            "books Maria 0.545227\n",
            "eating apple 1.813738\n",
            "eating eggs 1.120591\n",
            "eating things 1.120591\n",
            "eggs lot 2.730029\n",
            "eggs eating 1.120591\n",
            "eggs loves 0.532805\n",
            "fact loves 1.631417\n",
            "fact Maria 0.832909\n",
            "lot eggs 2.730029\n",
            "loves fact 1.631417\n",
            "loves things 1.225952\n",
            "loves eggs 0.532805\n",
            "novels read 1.120591\n",
            "novels books 0.650588\n",
            "novels Maria 0.427444\n",
            "read novels 1.120591\n",
            "read things 1.120591\n",
            "read books 0.832909\n",
            "things loves 1.225952\n",
            "things eating 1.120591\n",
            "things read 1.120591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPOjzTnBEqr8"
      },
      "source": [
        "# Word Similarity\n",
        "Given a word, we need to extract all word pairs with the given word, compute cosine similarity for all pairs and select the N most similar words.\n",
        "## Word Pairs\n",
        "The first process is word pair extraction. Given a specific word, for instance, `novel`, we build all pairs `<novel, word>` from the matrix file."
      ],
      "id": "NPOjzTnBEqr8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZWPlRORFiHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06b56c0-76ab-4075-8bc5-9e0a8a75db1a"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "target = \"novels\" ##target word\n",
        "w = defaultdict(int)\n",
        "\n",
        "##open input file matrix_pmi_filtered:\n",
        "file_matrix_pmi_filtered = open('matrix_pmi_filtered.txt', 'r')\n",
        "\n",
        "for line in file_matrix_pmi_filtered:  \n",
        "    line = line.strip() \n",
        "    line = line.split()\n",
        "    if len(line) >= 3:\n",
        "        word = line[0]\n",
        "      \n",
        "        w[word] += 1\n",
        "\n",
        "file2 = open('pairs.txt','w') ##output file\n",
        "sys.stdout = file2  ##print stdout is in the ouput file\n",
        "\n",
        "for word in sorted(w):\n",
        "    print ('%s %s' % (target, word))\n",
        "    print (target, word, file=sys.stderr)\n"
      ],
      "id": "4ZWPlRORFiHY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "novels Maria\n",
            "novels Pedro\n",
            "novels apple\n",
            "novels books\n",
            "novels eating\n",
            "novels eggs\n",
            "novels fact\n",
            "novels lot\n",
            "novels loves\n",
            "novels novels\n",
            "novels read\n",
            "novels things\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SDf-d6kIb5i"
      },
      "source": [
        "## Cosine Similarity\n",
        "Given the `pairs` file generated in the previous step and the final matrix (language model), the following script computes cosine similarity between each word pair"
      ],
      "id": "-SDf-d6kIb5i"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sxF97uRV2c0",
        "outputId": "8c626c2c-a2da-4e8d-b472-9651172f7c81"
      },
      "source": [
        "import math,sys\n",
        "from collections import defaultdict\n",
        "\n",
        "##open input file matrix_pmi_filtered:\n",
        "file_matrix_pmi_filtered = open('matrix_pmi_filtered.txt', 'r')\n",
        "##open pairs file:\n",
        "file_pairs = open('pairs.txt', 'r')\n",
        "\n",
        "##initializing dictionaries:\n",
        "dic = defaultdict(dict) \n",
        "w = defaultdict(float)\n",
        "simil = defaultdict(float)\n",
        "\n",
        "for line in file_matrix_pmi_filtered:  \n",
        "    line = line.strip()  \n",
        "    (word, context, weight) = line.split()\n",
        "    #print (word,context,weight, file=sys.stderr)\n",
        " \n",
        "    dic[word][context] = float(weight)\n",
        "    w[word] += float(weight)\n",
        "\n",
        "file2 = open('cosine.txt','w') ##output file\n",
        "sys.stdout = file2  ##print stdout is in the ouput file\n",
        "\n",
        "for line in file_pairs:\n",
        "  line = line.strip() \n",
        "  (w1,w2) = line.split()\n",
        "  common=0\n",
        "  wc_count1=0\n",
        "  wc_count2=0\n",
        "  w1_count = w[w1]\n",
        "  w2_count = w[w2]\n",
        "  \n",
        "  for context in dic[w1]:\n",
        "    wc_count1 += dic[w1][context] **2  #this builds the euclidean norm of word_1 vector\n",
        "\n",
        "  for context in dic[w2]:\n",
        "    wc_count2 += dic[w2][context] **2 #this builds the euclidean norm of word_2 vector\n",
        "    if context in dic[w1]:\n",
        "      common += dic[w1][context] * dic[w2][context]\n",
        "      #print (context,w1,w2, file=sys.stderr)\n",
        "\n",
        "  if (w1_count and w2_count):   \n",
        "    simil[w1,w2] = (common) / math.sqrt (wc_count1 * wc_count2)\n",
        "    print ('%s\\t%s\\t%.6f' %  (w1, w2, simil[w1,w2]))\n",
        "    print (w1, w2, simil[w1,w2], file=sys.stderr)\n"
      ],
      "id": "9sxF97uRV2c0",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "novels Maria 0.23996446673649857\n",
            "novels Pedro 0.09381105579484929\n",
            "novels apple 0.0\n",
            "novels books 0.7188325190384269\n",
            "novels eating 0.0\n",
            "novels eggs 0.0\n",
            "novels fact 0.1424489429187707\n",
            "novels lot 0.0\n",
            "novels loves 0.0\n",
            "novels novels 1.0\n",
            "novels read 0.22183111610835893\n",
            "novels things 0.4593344739127516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ6eLDd5axVt"
      },
      "source": [
        "## Ranking Top N Most Similar Words\n",
        "This script rank by similarity score the `N` (e.g., 3) the most similar words of the target word (e.g., `novels`)."
      ],
      "id": "IZ6eLDd5axVt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u07rtuXTbPwR",
        "outputId": "5f9ac5ec-879c-4928-b9a5-969b79f4eb0f"
      },
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "th = 3 #N most similar words\n",
        "\n",
        "##open cosine file:\n",
        "file_cosine = open('cosine.txt', 'r')\n",
        "\n",
        "w = defaultdict(dict)\n",
        "\n",
        "for line in file_cosine:  \n",
        "  line = line.strip()\n",
        "  print (line)\n",
        "  line = line.split('\\t') #In this case, the string separator is tabulation\n",
        "  \n",
        "  if len(line) >= 3:\n",
        "    word1 = line[0]\n",
        "    word2 = line[1]\n",
        "    simil =  float (line[2])\n",
        "    print (word1,word2,simil)\n",
        "    w[word1][word2] = simil\n",
        "\n",
        "file2 = open('ranking.txt','w') ##output file\n",
        "sys.stdout = file2  ##print stdout is in the ouput file\n",
        "\n",
        "for w1, second in sorted(w.items() ):\n",
        "  i=1\n",
        "  for w2 in sorted(second,key=second.get,reverse=True):\n",
        "    if i <= th:\n",
        "    #    print >> sys.stderr, \"threshold:\", th, i\n",
        "      print ('%s\\t%s\\t%.6f' % (w1, w2, w[w1][w2]))\n",
        "      print (w1,w2,w[w1][w2],file=sys.stderr)\n",
        "    i += 1\n"
      ],
      "id": "u07rtuXTbPwR",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "novels novels 1.0\n",
            "novels books 0.718833\n",
            "novels things 0.459334\n"
          ]
        }
      ]
    }
  ]
}